{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done...\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "print(\"Done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process of Formatting Data:\n",
    "1. Move all sentences into frequency feature vector, unigram model\n",
    "\n",
    "2. Move all tags into useable format for additive model accuracy\n",
    "\n",
    "3. Run data through one of the following models:\n",
    "\n",
    "    a. Recurrent Neural Network\n",
    "    \n",
    "    b. Heirarchical Clustering Method\n",
    "    \n",
    "    c. Decision Tree Regressor \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Moving Sentences into Binary Feature Vector, UNI & BI models\n",
    "#### Things made for training include... \n",
    "\n",
    "    descriptions_train\n",
    "    \n",
    "    features_train (useless)\n",
    "    \n",
    "    images_train\n",
    "    \n",
    "    tags_train\n",
    "\n",
    "#### Things made for testing include... \n",
    "    \n",
    "    descriptions_test\n",
    "    \n",
    "    features_trest\n",
    "    \n",
    "    images_test\n",
    "    \n",
    "    tags_test\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "train_descriptions = './all_data/descriptions_train'\n",
    "files = []\n",
    "for i in os.listdir(train_descriptions):\n",
    "    files.append(i)\n",
    "print(len(files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_data():\n",
    "    d = []\n",
    "    for f in files:\n",
    "        sentences_per_file = []\n",
    "        for line in open(train_descriptions+'/'+str(f)):\n",
    "            #stripping punctuation from the line\n",
    "            line = re.sub(r'[^\\w\\s]','',line)\n",
    "            #splitting the line into a list based on spaces\n",
    "            line = line.split(' ')\n",
    "            #removing the last two characters of the last item in the list\n",
    "            line[-1] = line[-1][:-2]\n",
    "            #appending the cleaned line to our array\n",
    "            sentences_per_file.append(line)\n",
    "        d.append(sentences_per_file)\n",
    "\n",
    "    #appending each chunk of sentences per file to our total raw train data\n",
    "    return d\n",
    "\n",
    "raw_train_descriptions = load_data()\n",
    "#printing length. Length should be equal to our number of files\n",
    "print(len(raw_train_descriptions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 382838)\n"
     ]
    }
   ],
   "source": [
    "#Initialized stemmer object and language\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "#Running list of stop words, can add more...\n",
    "stop_words = ['a', 'and', 'but', 'to', 'it', \n",
    "              'is', 'a', 'the', 'of', 'as', \n",
    "             'this', 'that', 'was']\n",
    "\n",
    "def stem():\n",
    "    '''\n",
    "    Returns a cleaned, stemmed list of lists in the same format\n",
    "    as the raw_train_descriptions. \n",
    "    Returns a list of all words seen in the dataset.\n",
    "    \n",
    "    Stemmer takes in a word like 'generously' and returns 'generous'\n",
    "    from online sources, snowball seems to be a favorite!\n",
    "    '''\n",
    "    cleaned_words = []\n",
    "    stemmed_train_descriptions = []\n",
    "    for chunk in raw_train_descriptions:\n",
    "        new_chunk = []\n",
    "        for sentence in chunk:\n",
    "            new_sentence = []\n",
    "            for word in sentence:\n",
    "                word = word.lower()\n",
    "                if word not in stop_words:\n",
    "                    clean = stemmer.stem(word)\n",
    "                    cleaned_words.append(clean)\n",
    "                    new_sentence.append(clean)\n",
    "            new_chunk.append(new_sentence)\n",
    "        stemmed_train_descriptions.append(new_chunk)\n",
    "    return stemmed_train_descriptions, cleaned_words\n",
    "\n",
    "stemmed_train, cleaned_words = stem()\n",
    "\n",
    "print(len(stemmed_train), len(cleaned_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7997\n",
      "LOL, only 8000 unique words!\n",
      "[u'skateboard', u'put', 'on', u'show', u'use', u'picnic', u'tabl', u'his', u'stag', u'skateboard']\n",
      "\n",
      "\n",
      "\n",
      "['', u'fourfiv', u'birdfeed', u'childern', u'woodi', u'yellow', u'four', u'sunlit', u'digit', u'deli']\n"
     ]
    }
   ],
   "source": [
    "#Getting a set() of the cleaned_words for the feature vector\n",
    "set_cleaned_words = list(set(cleaned_words))\n",
    "print(len(set_cleaned_words))\n",
    "print('LOL, only 8000 unique words!')\n",
    "\n",
    "print(cleaned_words[:10])\n",
    "print('\\n\\n') \n",
    "print(set_cleaned_words[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making a smaller dataframe where each 5 sentence reviews are combined into 1 list..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000\n",
      "[[u'skateboard', u'put', 'on', u'show', u'use', u'picnic', u'tabl', u'his', u'stag', u'skateboard', u'pull', u'trick', 'on', u'top', u'picnic', u'tabl', u'man', u'ride', 'on', u'skateboard', 'on', u'top', u'tabl', u'skate', u'boarder', u'do', u'trick', 'on', u'picnic', u'tabl', u'person', u'ride', u'skateboard', 'on', u'picnic', u'tabl', u'with', u'crowd', u'watchin'], [u'bowl', u'soup', u'has', u'some', u'carrot', u'shrimp', u'noodl', 'in', 'i', u'healthi', u'food', 'in', u'bowl', u'readi', u'eat', '', u'soup', u'has', u'carrot', u'shrimp', 'in', u'sit', u'next', u'chopstick', u'tasti', u'bowl', u'ramen', u'serv', u'for', u'someon', u'enjoy', '', u'bowl', u'asian', u'noodl', u'soup', u'with', u'shrimp', u'carrot']]\n"
     ]
    }
   ],
   "source": [
    "def condenser(list_of_lists):\n",
    "    total = []\n",
    "    for chunk in list_of_lists:\n",
    "        chunk_of_words = []\n",
    "        for sentence in chunk:\n",
    "            for word in sentence:\n",
    "                chunk_of_words.append(word)\n",
    "        total.append(chunk_of_words)\n",
    "    return total\n",
    "stemmed_train_per_image = condenser(stemmed_train)\n",
    "print(len(stemmed_train_per_image))\n",
    "print(stemmed_train_per_image[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Its Feature vector time!!! \n",
    "##### First creating a unigram vector, then a bigram vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fourfiv  birdfeed  childern  woodi  yellow  four  sunlit  digit  deli  \\\n",
      "0  0        0         0         0      0       0     0       0      0     0   \n",
      "1  2        0         0         0      0       0     0       0      0     0   \n",
      "2  2        0         0         0      0       0     0       0      0     0   \n",
      "3  1        0         0         0      0       0     0       0      0     0   \n",
      "4  2        0         0         0      0       0     0       0      0     0   \n",
      "\n",
      "   ...   humbl  cliff  pringl  tableclot  experienc  floret  tomot  pigeo  \\\n",
      "0  ...       0      0       0          0          0       0      0      0   \n",
      "1  ...       0      0       0          0          0       0      0      0   \n",
      "2  ...       0      0       0          0          0       0      0      0   \n",
      "3  ...       0      0       0          0          0       0      0      0   \n",
      "4  ...       0      0       0          0          0       0      0      0   \n",
      "\n",
      "   emerg  sash  \n",
      "0      0     0  \n",
      "1      0     0  \n",
      "2      0     0  \n",
      "3      0     0  \n",
      "4      0     0  \n",
      "\n",
      "[5 rows x 7997 columns]\n"
     ]
    }
   ],
   "source": [
    "unigram2 = pd.DataFrame(0, index = range(len(raw_train_descriptions)), \\\n",
    "                      columns = set_cleaned_words)\n",
    "#runtime ~~ 7 min\n",
    "def check_data(data, index):\n",
    "    return data[index]\n",
    "\n",
    "for col in unigram2.columns:\n",
    "#     print(col)\n",
    "    for row in range(len(unigram2)):\n",
    "        for word in stemmed_train_per_image[row]:\n",
    "            if col == word:\n",
    "                unigram2[col][row] += 1\n",
    "#         for word in stemmed_train_per_image[row]:\n",
    "#             if word == col:\n",
    "#                 unigram[col][row] += 1\n",
    "print(unigram2.head())\n",
    "unigram2.to_csv('unigram')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper / Scratch Work (for checking and confirming logic only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      fourfiv  birdfeed  childern  woodi  yellow  four  sunlit  digit  deli  \\\n",
      "0  0        0         0         0      0       0     0       0      0     0   \n",
      "1  1        0         0         0      0       0     0       0      0     0   \n",
      "2  1        0         0         0      0       0     0       0      0     0   \n",
      "3  1        0         0         0      0       0     0       0      0     0   \n",
      "4  1        0         0         0      0       0     0       0      0     0   \n",
      "\n",
      "   ...   humbl  cliff  pringl  tableclot  experienc  floret  tomot  pigeo  \\\n",
      "0  ...       0      0       0          0          0       0      0      0   \n",
      "1  ...       0      0       0          0          0       0      0      0   \n",
      "2  ...       0      0       0          0          0       0      0      0   \n",
      "3  ...       0      0       0          0          0       0      0      0   \n",
      "4  ...       0      0       0          0          0       0      0      0   \n",
      "\n",
      "   emerg  sash  \n",
      "0      0     0  \n",
      "1      0     0  \n",
      "2      0     0  \n",
      "3      0     0  \n",
      "4      0     0  \n",
      "\n",
      "[5 rows x 7997 columns]\n"
     ]
    }
   ],
   "source": [
    "# unigram = pd.DataFrame(0, index = range(len(raw_train_descriptions)), \\\n",
    "#                       columns = set_cleaned_words)\n",
    "# def check_data(data, index):\n",
    "#     return data[index]\n",
    "\n",
    "# for col in unigram.columns:\n",
    "# #     print(col)\n",
    "#     for row in range(len(unigram)):\n",
    "#         if col in stemmed_train_per_image[row]:\n",
    "#             unigram[col][row] += 1\n",
    "# #         for word in stemmed_train_per_image[row]:\n",
    "# #             if word == col:\n",
    "# #                 unigram[col][row] += 1\n",
    "# print(unigram.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 7997)\n",
      "(10000, 7997)\n"
     ]
    }
   ],
   "source": [
    "# print(unigram.shape)\n",
    "print(unigram2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 7997)\n",
      "True\n",
      "   a  b  c  aa  bb  cc\n",
      "0  0  0  0   0   0   0\n",
      "1  0  0  0   0   0   0\n",
      "2  0  0  0   0   0   0\n",
      "3  0  0  0   0   0   0\n",
      "4\n",
      "[['aa', 'bb', 'cc'], ['a'], ['b']]\n",
      "   a  b  c  aa  bb  cc\n",
      "0  0  0  0   0   0   0\n",
      "1  0  0  0   0   0   0\n",
      "2  0  0  0   0   0   0\n",
      "3  0  0  0   0   0   0\n",
      "44 done\n",
      "44 done\n"
     ]
    }
   ],
   "source": [
    "#Making sure im not crazy!\n",
    "# print(unigram.head())\n",
    "# print(unigram.shape)\n",
    "print('skateboard' in unigram2.columns)\n",
    "\n",
    "#Testing the gram model small scale...\n",
    "cols = ['a', 'b', 'c', 'aa', 'bb', 'cc']\n",
    "\n",
    "test_data = [[['a', 'a'], ['b', 'b'], ['cc']],\n",
    "             [['a'], ['b'], ['c']],\n",
    "             [['aa', 'bb', 'cc'], ['a'], ['b']],\n",
    "             [['a'], ['b'], ['c', 'cc', 'bb', 'aa']]]\n",
    "\n",
    "test_df = pd.DataFrame(0, index = range(len(test_data)), columns = cols)\n",
    "print(test_df)\n",
    "print(len(test_df))\n",
    "def check_data(data, index):\n",
    "    return data[index]\n",
    "print check_data(test_data, 2)\n",
    "\n",
    "# for col in test_df.columns:\n",
    "# #    print col\n",
    "#     for row in range(test_df.shape[0]):\n",
    "#         index_list = check_data(test_data, row)\n",
    "#         for i in index_list:\n",
    "#             for j in i:\n",
    "#                 if j == col:\n",
    "#                     test_df[col][row] += 1\n",
    "        \n",
    "\n",
    "print(test_df)\n",
    "\n",
    "g = 0\n",
    "for i in unigram['boarder']:\n",
    "    if i == 1:\n",
    "        g += 1\n",
    "print g, 'done'\n",
    "g = 0\n",
    "for i in unigram2['boarder']:\n",
    "    if i != 0:\n",
    "        g += 1\n",
    "print g, 'done'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
